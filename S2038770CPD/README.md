# Cloud Project Implementation

To ensure a cost-effective and secure implementation of the application on AWS, the chosen approach focuses on leveraging a variety of AWS services tailored to specific requirements.

## EC2 Instance
Firstly, an Amazon EC2 instance is utilized to simulate the camera system responsible for image processing. The instance is selected and configured based on workload demands to optimize cost while ensuring efficient resource utilization. I plan to use it to store the images before uploading them to the s3.

## S3 Buckets
Next, I employed Amazon S3 buckets for storing image files uploaded by the EC2 instance. The implementation included: 
Lifecycle policies are implemented to manage the storage lifecycle of objects stored in the S3 buckets. Infrequently accessed data is transitioned to cost-effective storage classes like S3 Glacier using lifecycle policies. By automatically moving data to lower-cost storage tiers based on defined rules and criteria, lifecycle policies help optimize storage costs over time without compromising data availability or durability. This approach aligns with cost optimization strategies by ensuring that storage resources are utilized efficiently based on the access patterns and lifecycle of the data.
I configured my s3 bucket with Server Side Encryption (SSE) using AES256 encryption. By implementing SSE, data stored in S3 buckets remains encrypted at rest, providing an additional layer of security against unauthorized access or data breaches. This encryption mechanism ensures the confidentiality and integrity of data stored within the buckets, aligning with industry best practices and compliance requirements. 
Versioning is enabled on my S3 bucket to maintain multiple versions of objects stored within them. This feature provides protection against accidental deletion or modification of objects by preserving previous versions, thereby enhancing data durability and integrity. Versioning also facilitates data recovery and rollback processes, contributing to improved data management practices and mitigating the risk of data loss.


## Cloudfront CDN
I created a Cloudfront CDN (Content Delivery Network) in front of my S3 bucket in order to cache my data and help with availability. Cloudfront increases availability of resources especially if they are required far from where they are stored. It optimizes latency and improves access speeds for downloading resources such as the images stored in the bucket. It also helps with security by providing a layer of protection against DDoS attacks. I used the default settings for the Cloudfront distribution. I also used the Cloudfront distribution to create a signed URL for the images in the bucket. This is a URL that is only valid for a certain amount of time and can be used to access the image. This is useful for when you want to share the image with someone but do not want them to have access to the image forever. I set the expiration time to 1 hour. I also set the policy to allow only GET requests. This is because I only want the image to be viewed and not modified. I also set the key pair ID to the one that was generated by AWS for me. I then used the following command to generate the signed URL:

I used boto3 python code to create the EC2 instance and therefore did not save the private key. When it came to uploading the images so that I can then upload them from EC2 to S3, I used wget. I first downloaded the zipped image file from GCU Learn then uploaded it to Google Drive and made it available to anyone with the link. I then used the following command on my ec2 in order to download it: 
wget "https://drive.google.com/uc?id=1ZvkW3a02woJJ_3DL7C5pfmJBFulVaPbF" -O images.zip. I then unzipped the file to get the folder with images. I also had to download pip and then boto3 on my EC2 instance in order to run the code that uploads the images to the s3 bucket. Even after that, I needed to either have credentials to access my s3 bucket, or update my EC2 Instance role to one that can access the buckets, such as LabRole. I chose the latter as it was simpler.

## SQS, Lambda, Rekognition, DynamoDB
For message handling and triggering of image analysis tasks, an Amazon SQS queue is integrated into the application architecture. SQS provides a scalable and cost-effective solution for decoupling components, ensuring seamless communication between services while optimizing resource usage. A Queue needs a destination that will poll it for messages and in this case I used an AWS Lambda. By creating a queue, I am creating an asynchronous workflow in the event that there are more messages (or files uploaded) in our queue than how many lambdas we have limited to run this process in parallel. Queues are good for one-to-one asynchronous communication between services, as a temporary message holding pool and ordered message processing that needs to be set up at queue construction. The FIFO system has limitations on the number of messages that can be in the Queue and thus other systems of message processing are the default.

AWS Lambda functions are employed for serverless computing, enabling on-demand execution of image analysis tasks triggered by messages from SQS. With Lambda's pay-as-you-go pricing model, the application only incurs costs proportional to the compute resources consumed during execution, leading to cost-efficient operations. I particularly employed Lambda functions in two instances. The first was in getting the data of the image uploaded to the s3 bucket from the data in the SQS Queue. As a destination and polling agent of the queue, the lambda retrieves important information such as the image name, the time of upload and the IP of the source of the image. 

AWS Rekognition is leveraged in the first lambda function for label and text detection in images uploaded to S3. With its pay-as-you-go pricing model, Rekognition ensures cost efficiency by charging only for the specific features utilized, aligning with the overall cost optimization strategy of the application. The lambda function then stores the image data, together with the detected texts and labels, in a dynamoDB entryTable.

To store image information and vehicle details, Amazon DynamoDB tables are utilized. Provisioned capacity mode with auto-scaling is implemented to optimize costs based on actual usage patterns, ensuring that resources are provisioned optimally to meet demand while minimizing unnecessary expenses. DynamoDB is a standard NoSQL database that scales horizontally. It has very fast performance, is very easy to use and very popular so it has a large community for support, in addition to being used by many applications and businesses. I created two tables. The “master” table is called vehicleTableS2038770 and it stores which vehicles are blacklisted and which ones are not. I only stored three of the four vehicles here, as I wanted to simulate the scenario of one unknown vehicle. In the three, only one was blacklisted.

Entry into the entry table by the first lambda triggers the second lambda which is used to check for blacklisted and unknown vehicles. It does this by scanning the vehicles table and comparing the text stored there with the text detected in the entry table row that was just entered. If blacklisted text, or an unknown vehicle is detected then an SNS topic is triggered and emails are sent to the subscribers.

The lambdas required setting up a role for access to SQS and dynamoDB and I chose LabRole for this.

## SNS
I used Amazon SNS for the email notification. SNS uses a Publisher-subscriber system where you own and publish to a topic and subscribers get notified of events that are delivered to that topic. A topic can have many subscribers (fan out approach) of different types (SQS, Lambda, Email). You should use an SNS if other people/entities care that an event has happened and so you need to notify them. It is like a newsletter. SNS allows asynchronous data processing by the different subscribers that offer a layer of protection, compared to sequential processing where if any one of the processes fail or data is corrupted in one of the processes, then the later processes will all be erroneous or end up failing all together. It is advisable that an SNS topic have SQS queues as subscribers instead of the actual processing endpoint. This is because if the endpoint fails, we will not need to depend on the retry mechanism available in SNS as a queue’s messages are available until the polling agent requests them. In my case, however, I could not use an SQS queue as a subscriber since the processing endpoint was an external agent, an email address, and thus could not poll an AWS SQS queue. I also included an s3 link to the concerned image in the email sent.

In the email, I included a link to an API generated using API gateway that sends an SMS notification, using an SNS topic subscription, requesting for assistance then displays emergency contacts when clicked. This is in case the email recipient wants to call for help. The technical implementation is that the API link triggers a lambda that notifies the SMS SNS topic and then returns a response with emergency contacts.

## IAM
I used the IAM access analyzer to generate my access policies and roles. I then limited them according to the least privilege permissions. This, for instance, allows only my EC2 and SQS to have access to the data stored in my S3 regardless of the roles assigned to either of them. This is a security measure to ensure that only the necessary services have access to the data stored in the S3 bucket. I also used the IAM access analyzer to generate my access policies and roles. I then limited them according to the least privilege permissions. This, for instance, allows only my EC2 and SQS to have access to the data stored in my S3 regardless of the roles assigned to either of them. This is a security measure to ensure that only the necessary services have access to the data stored in the S3 bucket. I also used the IAM access analyzer to generate my access policies and roles. I then limited them according to the least privilege permissions. This, for instance, allows only my EC2 and SQS to have access to the data stored in my S3 regardless of the roles assigned to either of them. This is a security measure to ensure that only the necessary services have access to the data stored in the S3 bucket.

## Conclusion
Overall, by strategically selecting and configuring AWS services tailored to the application's requirements, the approach ensures a cost-effective and secure implementation on the AWS platform. This combination of services enables the application to operate efficiently while adhering to best practices in both cost optimization and security.
